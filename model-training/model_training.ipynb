{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "from feat import Detector\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the detector\n",
    "\n",
    "detector = Detector(\n",
    "    face_model=\"retinaface\",\n",
    "    landmark_model=\"mobilefacenet\",\n",
    "    au_model='xgb',\n",
    "    emotion_model=\"resmasknet\",\n",
    "    facepose_model=\"img2pose\",\n",
    "    device=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DataFrame to store AU data\n",
    "aus_data = pd.DataFrame()\n",
    "\n",
    "# Define the path to the dataset directory\n",
    "dataset_path = \"../Desktop/dataset\"\n",
    "\n",
    "# Loop through each emotion subfolder in the dataset directory\n",
    "for emotion_folder in os.listdir(dataset_path):\n",
    "    emotion_path = os.path.join(dataset_path, emotion_folder)\n",
    "\n",
    "    # Check if it's a directory\n",
    "    if os.path.isdir(emotion_path):\n",
    "        # Process each image in the subfolder\n",
    "        for filename in glob.glob(os.path.join(emotion_path, \"*.png\")):\n",
    "            img = cv2.imread(filename)\n",
    "            detection = detector.detect_image(filename)\n",
    "\n",
    "            # Extract AU data\n",
    "            aus = detection.aus\n",
    "            au_data = pd.DataFrame(aus)\n",
    "\n",
    "            # Add emotion label from folder name\n",
    "            au_data['emotion'] = emotion_folder\n",
    "\n",
    "            # Rearrange columns to have 'emotion' first\n",
    "            cols = ['emotion'] + [col for col in au_data.columns if col != 'emotion']\n",
    "            au_data = au_data[cols]\n",
    "\n",
    "            # Append to the main DataFrame\n",
    "            aus_data = pd.concat([aus_data, au_data], ignore_index=True)\n",
    "\n",
    "# Save the AU data to a CSV file\n",
    "aus_data.to_csv(\"aus.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('aus.csv')\n",
    "\n",
    "# Ensure that only numeric columns are included in the mean calculation\n",
    "numeric_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].mean())\n",
    "\n",
    "# Encoding the 'emotion' column\n",
    "label_encoder = LabelEncoder()\n",
    "data['emotion'] = label_encoder.fit_transform(data['emotion'])\n",
    "\n",
    "# Feature Scaling for AUs\n",
    "scaler = StandardScaler()\n",
    "AU_columns = [col for col in data.columns if col.startswith('AU')]\n",
    "data[AU_columns] = scaler.fit_transform(data[AU_columns])\n",
    "\n",
    "# Split the dataset into train/validation/test sets (70/20/10 split)\n",
    "labels = data['emotion']\n",
    "features = data.drop('emotion', axis=1)\n",
    "\n",
    "# Splitting the data\n",
    "train_val_features, test_features, train_val_labels, test_labels = train_test_split(\n",
    "    features, labels, test_size=0.1, random_state=42, stratify=labels)\n",
    "train_features, val_features, train_labels, val_labels = train_test_split(\n",
    "    train_val_features, train_val_labels, test_size=(0.2/0.9), random_state=42, stratify=train_val_labels)\n",
    "\n",
    "# Grid search for SVM\n",
    "param_grid = {\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy', return_train_score=True)\n",
    "grid_search.fit(train_features, train_labels)\n",
    "\n",
    "# Best parameters and score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Evaluate on test set\n",
    "best_svm_model = grid_search.best_estimator_\n",
    "test_accuracy = best_svm_model.score(test_features, test_labels)\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Training Score: {best_score}\")\n",
    "print(f\"Test Set Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Save the trained model to a file\n",
    "with open('svm_model.pkl', 'wb') as file:\n",
    "    pickle.dump(best_svm_model, file)\n",
    "\n",
    "# Optionally, you can also save the scaler and label encoder if you need them for preprocessing in the future\n",
    "with open('scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)\n",
    "\n",
    "with open('label_encoder.pkl', 'wb') as file:\n",
    "    pickle.dump(label_encoder, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the file\n",
    "with open('svm_model.pkl', 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "\n",
    "# Load the scaler and label encoder if needed\n",
    "with open('scaler.pkl', 'rb') as file:\n",
    "    loaded_scaler = pickle.load(file)\n",
    "\n",
    "with open('label_encoder.pkl', 'rb') as file:\n",
    "    loaded_label_encoder = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'angry'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Assuming 'new_au_data' is your extracted AU features from Py-FEAT\n",
    "new_au_data_scaled = loaded_scaler.transform(new_au_data)  # Use the same scaler as during training\n",
    "predictions = loaded_model.predict(new_au_data_scaled)\n",
    "predicted_emotions = loaded_label_encoder.inverse_transform(predictions)\n",
    "\n",
    "predicted_emotions[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
